#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LinkedIn Jobs Dataset - contentSource Column Comprehensive Analysis
ContentSource sÃ¼tununun derinlemesine analizi: format, tutarlÄ±lÄ±k, veri tipi uyumluluÄŸu, diÄŸer sÃ¼tunlarla benzerlik
"""

import pandas as pd
import numpy as np
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

def analyze_contentSource_comprehensive(df):
    """contentSource sÃ¼tunu iÃ§in kapsamlÄ± analiz"""
    
    print("ğŸ” LINKEDIN JOBS DATASET - CONTENTSOURCE COLUMN COMPREHENSIVE ANALYSIS")
    print("=" * 75)
    
    column_name = 'contentSource'
    
    # Check if column exists
    if column_name not in df.columns:
        print(f"âŒ HATA: {column_name} sÃ¼tunu bulunamadÄ±!")
        print(f"ğŸ“‹ Mevcut sÃ¼tunlar: {list(df.columns)}")
        return None
    
    col_data = df[column_name]
    
    # 1. SÃœTUN TEMSÄ°LÄ° VE ANLAMI
    print("ğŸ—ï¸ 1. SÃœTUN TEMSÄ°LÄ° VE ANLAMI")
    print("-" * 35)
    
    column_info = {
        'name': column_name,
        'purpose': 'Job Content Source Classification',
        'content_type': 'LinkedIn Job Source Type Identifier',
        'business_value': 'Job posting source analysis, premium vs organic classification',
        'expected_format': 'Categorical string values (JOBS_PREMIUM, JOBS_PREMIUM_OFFLINE, JOBS_CREATE)',
        'source': 'LinkedIn Jobs API - Content source metadata',
        'data_usage': 'Job quality analysis, premium content identification, source filtering',
        'api_context': 'LinkedIn backend job processing pipeline identifier'
    }
    
    print(f"ğŸ“Š {column_info['name']}:")
    print(f"   ğŸ“ Purpose: {column_info['purpose']}")
    print(f"   ğŸ’¼ Content Type: {column_info['content_type']}")
    print(f"   ğŸ¯ Business Value: {column_info['business_value']}")
    print(f"   ğŸ“‹ Expected Format: {column_info['expected_format']}")
    print(f"   ğŸ”— Source: {column_info['source']}")
    print(f"   ğŸ¨ Usage: {column_info['data_usage']}")
    print(f"   ğŸ”§ API Context: {column_info['api_context']}")
    print()
    
    # 2. TEMEL Ä°STATÄ°STÄ°KLER
    print("ğŸ“Š 2. TEMEL Ä°STATÄ°STÄ°KLER")
    print("-" * 25)
    
    total_rows = len(col_data)
    null_count = col_data.isnull().sum()
    non_null_count = total_rows - null_count
    null_percentage = (null_count / total_rows) * 100
    unique_count = len(col_data.dropna().unique()) if non_null_count > 0 else 0
    memory_usage = col_data.memory_usage(deep=True) / 1024**2
    
    basic_stats = {
        'total_rows': total_rows,
        'null_count': null_count,
        'non_null_count': non_null_count,
        'null_percentage': null_percentage,
        'unique_count': unique_count,
        'memory_mb': memory_usage,
        'data_type': str(col_data.dtype)
    }
    
    print(f"ğŸ“ˆ Temel Ä°statistikler:")
    print(f"   ğŸ“Š Toplam kayÄ±t: {total_rows:,}")
    print(f"   âŒ Null: {null_count:,} ({null_percentage:.1f}%)")
    print(f"   âœ… Dolu: {non_null_count:,} ({100-null_percentage:.1f}%)")
    print(f"   ğŸ¯ Benzersiz deÄŸer: {unique_count:,}")
    print(f"   ğŸ’¾ Memory kullanÄ±mÄ±: {memory_usage:.3f} MB")
    print(f"   ğŸ”§ Mevcut veri tipi: {col_data.dtype}")
    print()
    
    if non_null_count == 0:
        print("âŒ HATA: HiÃ§ veri yok, analiz durduruluyor!")
        return None
    
    # 3. VERÄ° TÄ°PÄ° UYUMLULUÄU KONTROLÃœ
    print("âš™ï¸ 3. VERÄ° TÄ°PÄ° UYUMLULUÄU KONTROLÃœ")
    print("-" * 35)
    
    non_null_data = col_data.dropna()
    current_dtype = str(col_data.dtype)
    
    # Check if all values are strings
    all_strings = all(isinstance(val, str) for val in non_null_data)
    has_numbers = any(str(val).isdigit() for val in non_null_data)
    has_mixed = any(bool(re.search(r'\d', str(val))) and bool(re.search(r'[a-zA-Z]', str(val))) 
                   for val in non_null_data)
    
    print(f"ğŸ“ Mevcut veri tipi: {current_dtype}")
    print(f"ğŸ”¤ TÃ¼m deÄŸerler string: {'âœ… Evet' if all_strings else 'âŒ HayÄ±r'}")
    print(f"ğŸ”¢ SayÄ± iÃ§eren deÄŸerler: {'âš ï¸ Var' if has_numbers else 'âœ… Yok'}")
    print(f"ğŸ”¤ KarÄ±ÅŸÄ±k iÃ§erik: {'âš ï¸ Var' if has_mixed else 'âœ… Yok'}")
    
    # Recommended data type
    if unique_count <= 20:
        print(f"ğŸ’¡ Ã–NERÄ°: 'category' tipine Ã§evrilebilir!")
        print(f"   ğŸ¯ Sebep: Sadece {unique_count} benzersiz deÄŸer var")
        print(f"   ğŸ’¾ Memory KazancÄ±: %50-90 memory tasarrufu bekleniyor")
        recommended_type = 'category'
    else:
        print(f"ğŸ’¡ Ã–NERÄ°: 'object' tipi uygun")
        print(f"   ğŸ¯ Sebep: {unique_count} benzersiz deÄŸer, category iÃ§in fazla")
        recommended_type = 'object'
    
    print(f"ğŸ¯ Recommended Type: {recommended_type}")
    print()
    
    # 4. Ä°Ã‡ERÄ°K TUTARLILÄ°K ANALÄ°ZÄ°
    print("ğŸ” 4. Ä°Ã‡ERÄ°K TUTARLILÄ°K ANALÄ°ZÄ°")
    print("-" * 30)
    
    # Value distribution analysis
    value_counts = non_null_data.value_counts()
    value_distribution = (non_null_data.value_counts(normalize=True) * 100).round(2)
    
    print(f"ğŸ“‹ DeÄŸer DaÄŸÄ±lÄ±mÄ± ({unique_count} benzersiz deÄŸer):")
    for value, count in value_counts.head(10).items():
        percentage = value_distribution[value]
        print(f"   ğŸ“Š '{value}': {count:,} kayÄ±t ({percentage:.1f}%)")
    
    if len(value_counts) > 10:
        print(f"   ğŸ“ ... ve {len(value_counts)-10} deÄŸer daha")
    print()
    
    # 5. VARIANCE ANALÄ°ZÄ° (Ã–NEMLÄ°!)
    print("ğŸ¯ 5. VARIANCE ANALÄ°ZÄ° (KRÄ°TÄ°K!)")
    print("-" * 30)
    
    if unique_count == 1:
        print("ğŸš¨ KRÄ°TÄ°K: ZERO VARIANCE DETECTED!")
        print(f"   ğŸ“Š Tek deÄŸer: '{value_counts.index[0]}'")
        print(f"   âŒ Analitik deÄŸer: NONE")
        print(f"   ğŸ’¡ Ã–NERÄ°: DELETE column (zero variance)")
        variance_issue = "ZERO_VARIANCE"
    elif unique_count <= 3:
        print("âš ï¸ DÄ°KKAT: Very Low Variance!")
        print(f"   ğŸ“Š Sadece {unique_count} deÄŸer var")
        print(f"   ğŸ¯ Category conversion HIGHLY recommended")
        variance_issue = "LOW_VARIANCE"
    else:
        print("âœ… Ä°yi: Normal variance seviyesi")
        variance_issue = "NORMAL"
    print()
    
    # 6. FORMAT STANDARDÄ°ZASYON ANALÄ°ZÄ°
    print("ğŸ“ 6. FORMAT STANDARDÄ°ZASYON ANALÄ°ZÄ°")
    print("-" * 35)
    
    format_issues = []
    
    # Case analysis
    has_lowercase = any(val.islower() for val in non_null_data if isinstance(val, str))
    has_uppercase = any(val.isupper() for val in non_null_data if isinstance(val, str))
    has_mixed_case = any(val != val.upper() and val != val.lower() for val in non_null_data if isinstance(val, str))
    
    print(f"ğŸ“‹ Case Analizi:")
    print(f"   ğŸ”¤ Lowercase var: {'âš ï¸ Evet' if has_lowercase else 'âœ… HayÄ±r'}")
    print(f"   ğŸ”  Uppercase var: {'âœ… Evet' if has_uppercase else 'âš ï¸ HayÄ±r'}")
    print(f"   ğŸ”¤ Mixed case var: {'âš ï¸ Evet' if has_mixed_case else 'âœ… HayÄ±r'}")
    
    # Special characters analysis
    special_chars = set()
    for val in non_null_data:
        if isinstance(val, str):
            special_chars.update(char for char in val if not char.isalnum())
    
    print(f"ğŸ“‹ Ã–zel Karakter Analizi:")
    print(f"   ğŸ”§ Bulunan Ã¶zel karakterler: {sorted(special_chars) if special_chars else 'YOK'}")
    
    # Whitespace analysis
    has_leading_spaces = any(str(val).startswith(' ') for val in non_null_data)
    has_trailing_spaces = any(str(val).endswith(' ') for val in non_null_data)
    has_multiple_spaces = any('  ' in str(val) for val in non_null_data)
    
    print(f"ğŸ“‹ Whitespace Analizi:")
    print(f"   â¬…ï¸ Leading spaces: {'âš ï¸ Var' if has_leading_spaces else 'âœ… Yok'}")
    print(f"   â¡ï¸ Trailing spaces: {'âš ï¸ Var' if has_trailing_spaces else 'âœ… Yok'}")
    print(f"   ğŸ”„ Multiple spaces: {'âš ï¸ Var' if has_multiple_spaces else 'âœ… Yok'}")
    
    if has_lowercase or has_mixed_case or special_chars or has_leading_spaces or has_trailing_spaces or has_multiple_spaces:
        print(f"\nğŸ’¡ FORMAT STANDARDÄ°ZASYON Ã–NERÄ°SÄ°:")
        if has_lowercase or has_mixed_case:
            print(f"   ğŸ”  Case standardization: UPPERCASE Ã¶neriliyor")
        if has_leading_spaces or has_trailing_spaces:
            print(f"   âœ‚ï¸ Whitespace cleanup: strip() iÅŸlemi")
        if has_multiple_spaces:
            print(f"   ğŸ”„ Multiple space normalization gerekli")
    else:
        print(f"\nâœ… Format: Standardizasyon gerekmez")
    print()
    
    # 7. BOÅ ALAN YoÄUNLUÄU VE DOLDURMA Ã–NERÄ°LERÄ°
    print("ğŸ•³ï¸ 7. BOÅ ALAN YOÄUNLUÄU VE DOLDURMA Ã–NERÄ°LERÄ°")
    print("-" * 45)
    
    if null_count > 0:
        print(f"ğŸ“Š Null oranÄ±: {null_percentage:.1f}% ({null_count:,} kayÄ±t)")
        
        if null_percentage < 5:
            print("âœ… DÃ¼ÅŸÃ¼k null oranÄ± - problematik deÄŸil")
            fill_recommendation = "NOT_REQUIRED"
        elif null_percentage < 15:
            print("âš ï¸ Orta seviye null oranÄ±")
            fill_recommendation = "OPTIONAL"
        else:
            print("ğŸš¨ YÃ¼ksek null oranÄ± - dikkate alÄ±nmalÄ±")
            fill_recommendation = "RECOMMENDED"
        
        print(f"\nğŸ’¡ NULL DOLDURMA Ã–NERÄ°LERÄ°:")
        print(f"   ğŸ¯ Ä°ÅŸ Logic: En yaygÄ±n deÄŸer ile doldur")
        if len(value_counts) > 0:
            most_common = value_counts.index[0]
            most_common_pct = value_distribution.iloc[0]
            print(f"   ğŸ“Š En yaygÄ±n: '{most_common}' ({most_common_pct:.1f}%)")
        print(f"   ğŸ”§ Alternative: 'UNKNOWN' ya da 'NOT_SPECIFIED' deÄŸeri")
        print(f"   âš ï¸ Risk: Business logic'i deÄŸiÅŸtirebilir")
    else:
        print("âœ… Null deÄŸer yok - mÃ¼kemmel!")
        fill_recommendation = "NOT_NEEDED"
    print()
    
    # 8. DÄ°ÄER SÃœTUNLARLA BENZERLÄ°K KONTROLÃœ
    print("ğŸ” 8. DÄ°ÄER SÃœTUNLARLA BENZERLÄ°K KONTROLÃœ")
    print("-" * 40)
    
    similar_columns = []
    potential_source_columns = []
    
    # Check for similar column names or content patterns
    for other_col in df.columns:
        if other_col != column_name:
            # Name similarity check
            if ('source' in other_col.lower() or 
                'content' in other_col.lower() or 
                'type' in other_col.lower() or
                'category' in other_col.lower() or
                'premium' in other_col.lower() or
                'jobs' in other_col.lower()):
                
                potential_source_columns.append(other_col)
    
    print(f"ğŸ” Potansiyel benzer sÃ¼tunlar:")
    if potential_source_columns:
        for col in potential_source_columns[:10]:  # Show first 10
            print(f"   ğŸ“ {col}")
        if len(potential_source_columns) > 10:
            print(f"   ğŸ“ ... ve {len(potential_source_columns)-10} sÃ¼tun daha")
        
        print(f"\nğŸ’¡ BENZERLÄ°K ANALÄ°ZÄ° Ã–NERÄ°SÄ°:")
        print(f"   ğŸ” Bu sÃ¼tunlarÄ±n iÃ§eriÄŸini detaylÄ± karÅŸÄ±laÅŸtÄ±r")
        print(f"   ğŸ“Š Value overlap oranlarÄ±nÄ± hesapla")
        print(f"   ğŸ¯ EÄŸer >80% overlap varsa, duplicate possibility")
    else:
        print("   âœ… Benzer sÃ¼tun bulunamadÄ±")
    print()
    
    # 9. SÃœTUN Ã‡OÄULLAÅMASÄ± KONTROLÃœ  
    print("ğŸ”„ 9. SÃœTUN Ã‡OÄULLAÅMASI KONTROLÃœ")
    print("-" * 35)
    
    # Check for numbered variations or namespace patterns
    related_columns = []
    base_name = column_name.split('/')[0] if '/' in column_name else column_name
    
    for col in df.columns:
        if (base_name in col and col != column_name) or \
           (column_name in col and col != column_name):
            related_columns.append(col)
    
    if related_columns:
        print(f"ğŸ” Ä°lgili sÃ¼tunlar bulundu:")
        for col in related_columns:
            print(f"   ğŸ“ {col}")
        
        print(f"\nğŸ’¡ Ã‡OÄULLAÅMA ANALÄ°ZÄ° Ã–NERÄ°SÄ°:")
        print(f"   ğŸ” Array/list pattern check et")
        print(f"   ğŸ“Š Content distribution analiz et")
        print(f"   ğŸ¯ Consolidation opportunity'leri araÅŸtÄ±r")
    else:
        print("âœ… Ã‡oÄŸullaÅŸmÄ±ÅŸ sÃ¼tun bulunamadÄ±")
    print()
    
    # 10. STRATEJÄ°K DEÄERLENDÄ°RME VE Ã–NERÄ°LER
    print("ğŸ¯ 10. STRATEJÄ°K DEÄERLENDÄ°RME VE Ã–NERÄ°LER")
    print("-" * 45)
    
    recommendations = []
    
    # Data type optimization
    if recommended_type == 'category':
        recommendations.append("ğŸ”§ DATA TYPE: 'category' tipine Ã§evir (memory optimization)")
    
    # Variance issue handling
    if variance_issue == "ZERO_VARIANCE":
        recommendations.append("ğŸš¨ DELETE: Zero variance - analitik deÄŸer yok")
    elif variance_issue == "LOW_VARIANCE":
        recommendations.append("âš ï¸ CONSIDER: Low variance - consolidation opportunity")
    
    # Null handling
    if fill_recommendation in ["RECOMMENDED", "OPTIONAL"]:
        recommendations.append(f"ğŸ•³ï¸ NULL FILL: {fill_recommendation} - business logic ile doldur")
    
    # Format standardization
    if format_issues:
        recommendations.append("ğŸ“ FORMAT: Standardization gerekli")
    
    print(f"ğŸ“‹ Ã–ncelikli Ã–neriler:")
    if recommendations:
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
    else:
        print("   âœ… SÃ¼tun optimize edilmiÅŸ durumda")
    
    print(f"\nğŸ¯ OVERALL ASSESSMENT:")
    
    # Calculate overall quality score
    quality_score = 0
    if null_percentage < 5: quality_score += 25
    elif null_percentage < 15: quality_score += 15
    elif null_percentage < 25: quality_score += 5
    
    if variance_issue == "NORMAL": quality_score += 25
    elif variance_issue == "LOW_VARIANCE": quality_score += 15
    elif variance_issue == "ZERO_VARIANCE": quality_score += 0
    
    if unique_count <= 20: quality_score += 20  # Good for category
    elif unique_count <= 100: quality_score += 15
    else: quality_score += 10
    
    if len(format_issues) == 0: quality_score += 15
    else: quality_score += max(0, 15 - len(format_issues) * 3)
    
    if recommended_type == 'category': quality_score += 15
    else: quality_score += 10
    
    print(f"   ğŸ“Š Quality Score: {quality_score}/100")
    
    if quality_score >= 80:
        overall_status = "EXCELLENT"
        action = "OPTIMIZE for category type"
    elif quality_score >= 60:
        overall_status = "GOOD" 
        action = "MINOR optimizations needed"
    elif quality_score >= 40:
        overall_status = "PROBLEMATIC"
        action = "SIGNIFICANT improvements required"
    else:
        overall_status = "CRITICAL"
        action = "CONSIDER DELETION or major restructuring"
    
    print(f"   ğŸ¯ Status: {overall_status}")
    print(f"   ğŸ’¡ Action: {action}")
    print()
    
    return {
        'column_name': column_name,
        'total_rows': total_rows,
        'null_count': null_count,
        'null_percentage': null_percentage,
        'unique_count': unique_count,
        'memory_mb': memory_usage,
        'data_type': current_dtype,
        'recommended_type': recommended_type,
        'variance_issue': variance_issue,
        'quality_score': quality_score,
        'overall_status': overall_status,
        'action_required': action,
        'recommendations': recommendations,
        'format_issues': format_issues,
        'similar_columns': potential_source_columns,
        'related_columns': related_columns
    }

if __name__ == "__main__":
    # Load the dataset
    try:
        print("ğŸ“‚ Dataset yÃ¼kleniyor...")
        df = pd.read_csv('linkedin_jobs_dataset_insights_completed.csv')
        print(f"âœ… Dataset yÃ¼klendi: {len(df):,} kayÄ±t, {len(df.columns)} sÃ¼tun")
        print()
        
        # Run analysis
        result = analyze_contentSource_comprehensive(df)
        
        if result:
            print("âœ… Analiz tamamlandÄ±!")
        else:
            print("âŒ Analiz baÅŸarÄ±sÄ±z!")
            
    except FileNotFoundError:
        print("âŒ HATA: Dataset dosyasÄ± bulunamadÄ±!")
        print("ğŸ“‚ LÃ¼tfen 'linkedin_jobs_dataset_insights_completed.csv' dosyasÄ±nÄ±n var olduÄŸundan emin olun")
    except Exception as e:
        print(f"âŒ HATA: {str(e)}") 