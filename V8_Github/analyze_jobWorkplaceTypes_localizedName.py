#!/usr/bin/env python3
"""
LinkedIn Jobs Dataset Analysis: jobWorkplaceTypes/0/localizedName Column

Bu script jobWorkplaceTypes/0/localizedName sÃ¼tununu kapsamlÄ± olarak analiz eder:
1. Temel sÃ¼tun karakteristikleri
2. Veri kalitesi analizi  
3. Benzersiz deÄŸer daÄŸÄ±lÄ±mÄ±
4. DiÄŸer sÃ¼tunlarla benzerlik analizi
5. Redundancy ve cross-column korelasyon tespiti
"""

import pandas as pd
import numpy as np
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def load_dataset():
    """Dataset'i yÃ¼kle"""
    try:
        df = pd.read_csv('linkedin_jobs_cleaned_no_redundant_urn.csv')
        print(f"âœ… Dataset baÅŸarÄ±yla yÃ¼klendi: {len(df):,} kayÄ±t, {len(df.columns)} sÃ¼tun")
        return df
    except Exception as e:
        print(f"âŒ Dataset yÃ¼kleme hatasÄ±: {e}")
        return None

def analyze_target_column(df, target_col):
    """Target sÃ¼tunun temel analizi"""
    print("=" * 80)
    print(f"ğŸ¯ TARGET SÃœTUN ANALÄ°ZÄ°: {target_col}")
    print("=" * 80)
    
    # Temel istatistikler
    total_rows = len(df)
    non_null_count = df[target_col].count()
    null_count = df[target_col].isnull().sum()
    unique_count = df[target_col].nunique()
    
    print(f"ğŸ“Š TEMEL Ä°STATÄ°STÄ°KLER:")
    print(f"   â€¢ Veri Tipi: {df[target_col].dtype}")
    print(f"   â€¢ Toplam KayÄ±t: {total_rows:,}")
    print(f"   â€¢ Null Olmayan DeÄŸer: {non_null_count:,}")
    print(f"   â€¢ Null DeÄŸer: {null_count:,}")
    print(f"   â€¢ Veri TamlÄ±ÄŸÄ±: {(non_null_count/total_rows*100):.2f}%")
    print(f"   â€¢ Benzersiz DeÄŸer SayÄ±sÄ±: {unique_count:,}")
    print(f"   â€¢ Benzersizlik OranÄ±: {(unique_count/non_null_count*100):.2f}%")
    
    return {
        'total_rows': total_rows,
        'non_null_count': non_null_count, 
        'null_count': null_count,
        'unique_count': unique_count,
        'data_type': str(df[target_col].dtype)
    }

def analyze_unique_values(df, target_col):
    """Benzersiz deÄŸer analizi"""
    print(f"\nğŸ“ˆ BENZERSÄ°Z DEÄER ANALÄ°ZÄ°:")
    print("-" * 50)
    
    # Value counts
    value_counts = df[target_col].value_counts()
    print(f"Toplam benzersiz deÄŸer: {len(value_counts)}")
    
    print(f"\nğŸ”¢ DEÄER DAÄILIMI:")
    for i, (value, count) in enumerate(value_counts.items(), 1):
        percentage = (count / df[target_col].count()) * 100
        print(f"   {i:2d}. '{value}': {count:,} kayÄ±t ({percentage:.2f}%)")
    
    # Null analizi
    if df[target_col].isnull().sum() > 0:
        print(f"\nâ“ NULL DEÄER ANALÄ°ZÄ°:")
        print(f"   â€¢ Null kayÄ±tlar: {df[target_col].isnull().sum():,}")
        print(f"   â€¢ Null oranÄ±: {(df[target_col].isnull().sum()/len(df)*100):.2f}%")
    
    return value_counts

def find_similar_columns(df, target_col):
    """Benzer sÃ¼tunlarÄ± tespit et"""
    print(f"\nğŸ” BENZER SÃœTUN ARAMA:")
    print("-" * 50)
    
    target_unique_count = df[target_col].nunique()
    target_values = set(df[target_col].dropna().unique())
    
    similar_columns = []
    workplace_related_columns = []
    
    for col in df.columns:
        if col == target_col:
            continue
            
        # Workplace/work ile ilgili sÃ¼tunlarÄ± tespit et
        if any(keyword in col.lower() for keyword in ['work', 'workplace', 'office', 'remote', 'location', 'place']):
            workplace_related_columns.append(col)
        
        # Benzersiz deÄŸer sayÄ±sÄ± benzerliÄŸi
        col_unique_count = df[col].nunique()
        if col_unique_count == target_unique_count and col_unique_count < 10:  # DÃ¼ÅŸÃ¼k kardinalite
            col_values = set(df[col].dropna().unique())
            
            # DeÄŸer benzerliÄŸi kontrolÃ¼
            if len(col_values & target_values) > 0:  # Ortak deÄŸerler var mÄ±?
                jaccard_similarity = len(col_values & target_values) / len(col_values | target_values)
                similar_columns.append({
                    'column': col,
                    'unique_count': col_unique_count,
                    'jaccard_similarity': jaccard_similarity,
                    'common_values': col_values & target_values,
                    'all_values': col_values
                })
    
    print(f"ğŸ¢ WORKPLACE Ä°LE Ä°LGÄ°LÄ° SÃœTUNLAR ({len(workplace_related_columns)} adet):")
    for col in workplace_related_columns:
        unique_count = df[col].nunique()
        null_pct = (df[col].isnull().sum() / len(df)) * 100
        print(f"   â€¢ {col}: {unique_count:,} benzersiz deÄŸer, %{null_pct:.1f} null")
    
    print(f"\nğŸ¯ AYNI BENZERSÄ°Z DEÄER SAYISINA SAHÄ°P SÃœTUNLAR:")
    if similar_columns:
        for sim in similar_columns:
            print(f"   â€¢ {sim['column']}: {sim['unique_count']} benzersiz deÄŸer")
            print(f"     - Jaccard BenzerliÄŸi: {sim['jaccard_similarity']:.3f}")
            print(f"     - Ortak DeÄŸerler: {sim['common_values']}")
            print(f"     - TÃ¼m DeÄŸerler: {sim['all_values']}")
            print()
    else:
        print("   âŒ AynÄ± benzersiz deÄŸer sayÄ±sÄ±na sahip sÃ¼tun bulunamadÄ±")
    
    return similar_columns, workplace_related_columns

def cross_column_analysis(df, target_col, workplace_columns):
    """Cross-column detaylÄ± analiz"""
    print(f"\nğŸ”„ CROSS-COLUMN DETAYLI ANALÄ°Z:")
    print("-" * 50)
    
    # jobWorkplaceTypes ile ilgili diÄŸer sÃ¼tunlarÄ± bul
    workplace_type_columns = [col for col in df.columns if 'jobWorkplaceTypes' in col]
    
    print(f"ğŸ“‹ JOBWORKPLACETYPES SÃœTUN GRU BU ({len(workplace_type_columns)} adet):")
    for col in workplace_type_columns:
        unique_count = df[col].nunique()
        null_count = df[col].isnull().sum()
        null_pct = (null_count / len(df)) * 100
        print(f"   â€¢ {col}")
        print(f"     - Benzersiz deÄŸer: {unique_count:,}")
        print(f"     - Null: {null_count:,} (%{null_pct:.1f})")
        
        # EÄŸer benzersiz deÄŸer sayÄ±sÄ± dÃ¼ÅŸÃ¼kse, deÄŸerleri gÃ¶ster
        if unique_count <= 10:
            values = df[col].value_counts()
            print(f"     - DeÄŸerler: {dict(values)}")
        print()
    
    return workplace_type_columns

def redundancy_assessment(df, target_col, similar_columns, workplace_columns):
    """Redundancy deÄŸerlendirmesi"""
    print(f"\nâš ï¸  REDUNDANCY DEÄERLENDÄ°RMESÄ°:")
    print("-" * 50)
    
    target_unique_count = df[target_col].nunique()
    target_null_pct = (df[target_col].isnull().sum() / len(df)) * 100
    
    print(f"ğŸ¯ TARGET SÃœTUN: {target_col}")
    print(f"   â€¢ Benzersiz deÄŸer: {target_unique_count}")
    print(f"   â€¢ Null oranÄ±: %{target_null_pct:.1f}")
    
    # Potansiyel redundant sÃ¼tunlarÄ± tespit et
    redundant_candidates = []
    
    for col in workplace_columns:
        if col == target_col:
            continue
            
        col_unique = df[col].nunique()
        col_null_pct = (df[col].isnull().sum() / len(df)) * 100
        
        # AynÄ± benzersiz deÄŸer sayÄ±sÄ± ve dÃ¼ÅŸÃ¼k kardinalite
        if col_unique == target_unique_count and col_unique <= 5:
            # DeÄŸer benzerliÄŸi kontrolÃ¼
            target_values = set(df[target_col].dropna().unique())
            col_values = set(df[col].dropna().unique())
            
            if len(target_values & col_values) > 0:
                jaccard = len(target_values & col_values) / len(target_values | col_values)
                redundant_candidates.append({
                    'column': col,
                    'unique_count': col_unique,
                    'null_pct': col_null_pct,
                    'jaccard_similarity': jaccard,
                    'common_values': target_values & col_values
                })
    
    if redundant_candidates:
        print(f"\nğŸš¨ POTANSIYEL REDUNDANT SÃœTUNLAR:")
        for candidate in redundant_candidates:
            print(f"   â€¢ {candidate['column']}")
            print(f"     - Benzersiz deÄŸer: {candidate['unique_count']}")
            print(f"     - Null oranÄ±: %{candidate['null_pct']:.1f}")
            print(f"     - Jaccard benzerliÄŸi: {candidate['jaccard_similarity']:.3f}")
            print(f"     - Ortak deÄŸerler: {candidate['common_values']}")
            print()
    else:
        print("   âœ… Redundant sÃ¼tun tespit edilmedi")
    
    return redundant_candidates

def business_value_assessment(df, target_col, value_counts):
    """Ä°ÅŸ deÄŸeri deÄŸerlendirmesi"""
    print(f"\nğŸ’¼ Ä°Å DEÄERÄ° DEÄERLENDÄ°RMESÄ°:")
    print("-" * 50)
    
    unique_count = len(value_counts)
    
    print(f"ğŸ“Š WORKPLACE TYPES ANALÄ°ZÄ°:")
    print(f"   â€¢ Toplam workplace tÃ¼rÃ¼: {unique_count}")
    
    if unique_count <= 10:
        print(f"   â€¢ DÃ¼ÅŸÃ¼k kardinalite: Kategorik veri uygun")
        
        # Ä°ÅŸ deÄŸeri analizi
        print(f"\nğŸ¢ WORKPLACE TÃœRÃœ DAÄILIMI:")
        total_non_null = df[target_col].count()
        
        for i, (workplace_type, count) in enumerate(value_counts.items(), 1):
            percentage = (count / total_non_null) * 100
            print(f"   {i}. {workplace_type}")
            print(f"      - KayÄ±t sayÄ±sÄ±: {count:,}")
            print(f"      - Oran: %{percentage:.2f}")
            
            # Ä°ÅŸ insights
            if 'remote' in str(workplace_type).lower():
                print(f"      - ğŸ  Uzaktan Ã§alÄ±ÅŸma fÄ±rsatÄ±")
            elif 'office' in str(workplace_type).lower():
                print(f"      - ğŸ¢ Ofis bazlÄ± Ã§alÄ±ÅŸma")
            elif 'hybrid' in str(workplace_type).lower():
                print(f"      - ğŸ”„ Hibrit Ã§alÄ±ÅŸma modeli")
            print()
    
    # Business value score
    completeness_score = (df[target_col].count() / len(df)) * 100
    diversity_score = min(unique_count / 5 * 100, 100)  # 5 ideal kategorik Ã§eÅŸitlilik
    distribution_score = 100 - (value_counts.iloc[0] / df[target_col].count() * 100)  # DaÄŸÄ±lÄ±m dengesi
    
    business_score = (completeness_score * 0.4 + diversity_score * 0.3 + distribution_score * 0.3)
    
    print(f"ğŸ“ˆ Ä°Å DEÄERÄ° SKORU:")
    print(f"   â€¢ Veri TamlÄ±ÄŸÄ±: %{completeness_score:.1f}")
    print(f"   â€¢ Kategori Ã‡eÅŸitliliÄŸi: %{diversity_score:.1f}")
    print(f"   â€¢ DaÄŸÄ±lÄ±m Dengesi: %{distribution_score:.1f}")
    print(f"   â€¢ TOPLAM Ä°Å DEÄERÄ°: %{business_score:.1f}")
    
    return business_score

def generate_recommendations(df, target_col, redundant_candidates, business_score):
    """Ã–neriler oluÅŸtur"""
    print(f"\nğŸ’¡ STRATEJÄ°K Ã–NERÄ°LER:")
    print("=" * 50)
    
    # Veri kalitesi Ã¶nerileri
    null_pct = (df[target_col].isnull().sum() / len(df)) * 100
    unique_count = df[target_col].nunique()
    
    print(f"ğŸ¯ SÃœTUN: {target_col}")
    print(f"   â€¢ Ä°ÅŸ DeÄŸeri Skoru: %{business_score:.1f}")
    
    if business_score >= 80:
        print(f"   âœ… YÃœKSEKDeÄŸer - KorunmasÄ± Ã¶nerilir")
    elif business_score >= 60:
        print(f"   âš ï¸  ORTA DeÄŸer - Optimizasyon deÄŸerlendirilebilir")
    else:
        print(f"   âŒ DÃœÅÃœK DeÄŸer - Eliminasyon deÄŸerlendirilebilir")
    
    print(f"\nğŸ“‹ DETAYLI Ã–NERÄ°LER:")
    
    # Null handling
    if null_pct > 5:
        print(f"   â€¢ Null DeÄŸer Optimizasyonu: %{null_pct:.1f} null var")
        print(f"     - Veri kaynaÄŸÄ± kalitesini gÃ¶zden geÃ§irin")
        print(f"     - Default deÄŸer atama stratejisi deÄŸerlendirin")
    
    # Redundancy handling
    if redundant_candidates:
        print(f"   â€¢ Redundancy YÃ¶netimi:")
        for candidate in redundant_candidates:
            print(f"     - {candidate['column']} ile %{candidate['jaccard_similarity']*100:.1f} benzerlik")
            print(f"     - Consolidation deÄŸerlendirin")
    
    # Business insights
    if unique_count <= 5:
        print(f"   â€¢ Kategorik Analiz FÄ±rsatÄ±:")
        print(f"     - Workplace types segmentasyonu iÃ§in idealn")
        print(f"     - HR analytics iÃ§in deÄŸerli insight")
        print(f"     - Trend analizi yapÄ±labilir")
    
    print(f"\nğŸ“Š SONUÃ‡:")
    if business_score >= 70 and len(redundant_candidates) == 0:
        print(f"   âœ… KORUMA: SÃ¼tun optimize edilmiÅŸ durumda")
    elif len(redundant_candidates) > 0:
        print(f"   ğŸ”„ OPTÄ°MÄ°ZASYON: Redundancy eliminasyonu Ã¶nerilir")
    else:
        print(f"   âš ï¸  Ä°NCELEME: DetaylÄ± business value analizi yapÄ±n")

def main():
    """Ana analiz fonksiyonu"""
    print("ğŸš€ LinkedIn Jobs Dataset - jobWorkplaceTypes/0/localizedName SÃ¼tun Analizi")
    print("=" * 80)
    
    # Dataset yÃ¼kle
    df = load_dataset()
    if df is None:
        return
    
    target_col = 'jobWorkplaceTypes/0/localizedName'
    
    # SÃ¼tun varlÄ±ÄŸÄ±nÄ± kontrol et
    if target_col not in df.columns:
        print(f"âŒ Hata: '{target_col}' sÃ¼tunu bulunamadÄ±!")
        print(f"Mevcut sÃ¼tunlar: {[col for col in df.columns if 'workplace' in col.lower()]}")
        return
    
    # Analizleri gerÃ§ekleÅŸtir
    basic_stats = analyze_target_column(df, target_col)
    value_counts = analyze_unique_values(df, target_col)
    similar_columns, workplace_columns = find_similar_columns(df, target_col)
    workplace_type_columns = cross_column_analysis(df, target_col, workplace_columns)
    redundant_candidates = redundancy_assessment(df, target_col, similar_columns, workplace_columns)
    business_score = business_value_assessment(df, target_col, value_counts)
    generate_recommendations(df, target_col, redundant_candidates, business_score)
    
    print(f"\nâœ… Analiz tamamlandÄ±!")
    print(f"ğŸ“ SonuÃ§ dosyasÄ±: workplace_types_analysis_results.txt olarak kaydedilebilir")

if __name__ == "__main__":
    main() 