#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LinkedIn Jobs Dataset - Single Column Deep Analysis
company/followingState/preDashFollowingInfoUrn sÃ¼tunun kapsamlÄ± analizi
"""

import pandas as pd
import numpy as np
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

def analyze_predash_following_info_urn(df):
    """company/followingState/preDashFollowingInfoUrn sÃ¼tunun detaylÄ± analizi"""
    
    column_name = 'company/followingState/preDashFollowingInfoUrn'
    
    print(f"ğŸ” KAPSAMLI SÃœTUN ANALÄ°ZÄ°: {column_name}")
    print("=" * 80)
    
    if column_name not in df.columns:
        print(f"âŒ HATA: '{column_name}' sÃ¼tunu bulunamadÄ±!")
        available_cols = [col for col in df.columns if 'following' in col.lower()]
        if available_cols:
            print("ğŸ” Mevcut following sÃ¼tunlarÄ±:")
            for col in available_cols:
                print(f"   ğŸ“‹ {col}")
        return None
    
    col_data = df[column_name]
    
    # 1. SÃœTUN TEMSÄ°LÄ° VE ANLAMI
    print("ğŸ“‹ 1. SÃœTUN TEMSÄ°LÄ° VE ANLAMI")
    print("-" * 40)
    print("ğŸ”— Bu sÃ¼tun: LinkedIn Following State URN bilgisini temsil eder")
    print("   ğŸ“ Ä°Ã§erik: Company following durumu Ã¶ncesi URN (Uniform Resource Name) identifiers")
    print("   ğŸ¯ Business Value: User-company relationship tracking, engagement analysis")
    print("   ğŸ“Š Expected Type: Text/String (object) - URN format")
    print("   ğŸ’¼ Domain Context: LinkedIn API following state management")
    print("   ğŸ”§ Technical Purpose: Unique identifier for following relationships")
    print()
    
    # 2. TEMEL Ä°STATÄ°STÄ°KLER
    print("ğŸ“Š 2. TEMEL Ä°STATÄ°STÄ°KLER")
    print("-" * 30)
    total_rows = len(col_data)
    null_count = col_data.isnull().sum()
    non_null_count = total_rows - null_count
    null_percentage = (null_count / total_rows) * 100
    
    print(f"ğŸ“‹ Toplam kayÄ±t: {total_rows:,}")
    print(f"âŒ Null deÄŸer: {null_count:,} ({null_percentage:.1f}%)")
    print(f"âœ… Dolu deÄŸer: {non_null_count:,} ({100-null_percentage:.1f}%)")
    print(f"ğŸ”§ Mevcut veri tipi: {col_data.dtype}")
    print()
    
    # 3. VERÄ° TÄ°PÄ° UYGUNLUÄU KONTROLÃœ
    print("âš™ï¸ 3. VERÄ° TÄ°PÄ° UYGUNLUÄU KONTROLÃœ")
    print("-" * 35)
    
    non_null_data = col_data.dropna()
    if len(non_null_data) > 0:
        # URN format kontrolÃ¼
        urn_pattern = r'^urn:'
        valid_urn_format = non_null_data.str.match(urn_pattern, na=False).sum()
        urn_format_pct = (valid_urn_format / len(non_null_data)) * 100 if len(non_null_data) > 0 else 0
        
        # String format kontrolÃ¼
        all_strings = all(isinstance(val, str) for val in non_null_data)
        has_numbers = any(str(val).isdigit() for val in non_null_data)
        
        print(f"ğŸ“ TÃ¼m deÄŸerler string: {'âœ… Evet' if all_strings else 'âŒ HayÄ±r'}")
        print(f"ğŸ”— Valid URN format: {valid_urn_format:,} ({urn_format_pct:.1f}%)")
        print(f"ğŸ”¢ Sadece sayÄ± olan deÄŸerler: {'âš ï¸ Var' if has_numbers else 'âœ… Yok'}")
        
        # URN pattern analizi
        if len(non_null_data) > 0:
            sample_values = non_null_data.head(5).tolist()
            print(f"\nğŸ“‹ Ã–rnek deÄŸerler:")
            for i, val in enumerate(sample_values, 1):
                display_val = str(val)[:60] + "..." if len(str(val)) > 60 else str(val)
                print(f"   {i}. {display_val}")
        
        # Ã–nerilen veri tipi
        if urn_format_pct > 90:
            print(f"\nğŸ’¡ Ã–NERÄ°: 'object' tipi uygun (URN format detected)")
        elif urn_format_pct > 50:
            print(f"\nğŸ’¡ Ã–NERÄ°: 'object' tipi uygun ama format cleaning gerekli")
        else:
            print(f"\nâš ï¸ Ã–NERÄ°: Format inconsistency detected - data cleaning required")
    print()
    
    # 4. Ä°Ã‡ERÄ°K ANALÄ°ZÄ° VE TUTARLILIK
    print("ğŸ” 4. Ä°Ã‡ERÄ°K ANALÄ°ZÄ° VE TUTARLILIK")
    print("-" * 35)
    
    if len(non_null_data) > 0:
        # Benzersiz deÄŸer analizi
        unique_values = non_null_data.unique()
        unique_count = len(unique_values)
        value_counts = non_null_data.value_counts()
        
        print(f"ğŸ¯ Benzersiz deÄŸer sayÄ±sÄ±: {unique_count:,}")
        
        if unique_count > 0:
            print(f"ğŸ“Š En sÄ±k deÄŸer: '{value_counts.index[0][:50]}...' ({value_counts.iloc[0]:,} kez)")
            
            # Variance level assessment
            if unique_count == 1:
                variance_level = "ğŸ”´ ZERO VARIANCE"
            elif unique_count < 10:
                variance_level = "ğŸŸ¡ LOW VARIANCE" 
            elif unique_count < 100:
                variance_level = "ğŸŸ  MODERATE VARIANCE"
            else:
                variance_level = "ğŸŸ¢ HIGH VARIANCE"
            
            print(f"ğŸ“ˆ Variance Level: {variance_level}")
        
        # URN pattern deep analysis
        print(f"\nğŸ” URN Pattern Analysis:")
        
        # URN prefix analysis
        if len(non_null_data) > 0:
            # Extract URN prefixes
            urn_prefixes = []
            for val in non_null_data:
                if str(val).startswith('urn:'):
                    parts = str(val).split(':')
                    if len(parts) >= 3:
                        prefix = ':'.join(parts[:3])  # urn:li:something
                        urn_prefixes.append(prefix)
            
            if urn_prefixes:
                prefix_counts = Counter(urn_prefixes)
                print(f"   ğŸ”— URN prefixes found: {len(prefix_counts)}")
                for prefix, count in prefix_counts.most_common(5):
                    pct = (count / len(urn_prefixes)) * 100
                    print(f"      ğŸ“Š {prefix}: {count:,} ({pct:.1f}%)")
        
        # Uzunluk analizi
        if col_data.dtype == 'object':
            lengths = non_null_data.astype(str).str.len()
            print(f"\nğŸ“ Karakter uzunluÄŸu analizi:")
            print(f"   Min: {lengths.min()} | Max: {lengths.max()}")
            print(f"   Ortalama: {lengths.mean():.1f} | Medyan: {lengths.median():.1f}")
            
            # Standart URN uzunluÄŸu kontrolÃ¼
            std_length_range = (lengths >= 40) & (lengths <= 100)  # Typical URN range
            std_length_count = std_length_range.sum()
            std_length_pct = (std_length_count / len(lengths)) * 100
            
            print(f"   ğŸ¯ Standard URN length (40-100 char): {std_length_count:,} ({std_length_pct:.1f}%)")
            
            # Ã‡ok kÄ±sa veya Ã§ok uzun deÄŸerleri tespit et
            very_short = (lengths <= 10).sum()
            very_long = (lengths >= 150).sum()
            if very_short > 0:
                print(f"   âš ï¸ Ã‡ok kÄ±sa deÄŸerler (â‰¤10 karakter): {very_short}")
            if very_long > 0:
                print(f"   âš ï¸ Ã‡ok uzun deÄŸerler (â‰¥150 karakter): {very_long}")
    print()
    
    # 5. FORMAT TUTARSIZLIKLARI VE STANDARDIZATION
    print("ğŸ”§ 5. FORMAT TUTARSIZLIKLARI VE STANDARDIZATION")
    print("-" * 45)
    
    if len(non_null_data) > 0:
        # URN format consistency
        valid_urn_pattern = r'^urn:li:[a-zA-Z]+:\d+$'
        valid_urns = non_null_data.str.match(valid_urn_pattern, na=False).sum()
        valid_urn_pct = (valid_urns / len(non_null_data)) * 100
        
        print(f"ğŸ”— Valid LinkedIn URN format: {valid_urns:,} ({valid_urn_pct:.1f}%)")
        
        # Case sensitivity check
        case_variations = len(non_null_data.unique()) != len(non_null_data.str.lower().unique())
        print(f"ğŸ”¤ Case variations detected: {'âš ï¸ Yes' if case_variations else 'âœ… No'}")
        
        # Special character analysis
        special_chars = non_null_data.str.contains(r'[^a-zA-Z0-9:\-_]', regex=True, na=False).sum()
        print(f"ğŸ”§ Special characters: {special_chars:,} values")
        
        # Format inconsistency detection
        inconsistent_formats = []
        for val in non_null_data.head(20):  # Sample check
            val_str = str(val)
            if not val_str.startswith('urn:'):
                inconsistent_formats.append(val_str)
        
        if inconsistent_formats:
            print(f"âš ï¸ Format inconsistencies detected: {len(inconsistent_formats)} samples")
            print("   ğŸ“‹ Inconsistent samples:")
            for sample in inconsistent_formats[:3]:
                print(f"      â€¢ {sample[:50]}...")
        else:
            print("âœ… Format consistency: Good")
    print()
    
    # 6. DÄ°ÄER SÃœTUNLARLA Ä°LÄ°ÅKÄ° ANALÄ°ZÄ°
    print("ğŸ”— 6. DÄ°ÄER SÃœTUNLARLA Ä°LÄ°ÅKÄ° ANALÄ°ZÄ°")
    print("-" * 35)
    
    # followingState namespace analysis
    following_state_columns = [col for col in df.columns if 'followingState' in col]
    print(f"ğŸ” FollowingState namespace sÃ¼tunlarÄ±: {len(following_state_columns)}")
    
    for col in following_state_columns:
        if col != column_name:
            # Basic correlation analysis
            try:
                other_data = df[col].dropna()
                if len(other_data) > 0 and len(non_null_data) > 0:
                    # Check for shared null patterns
                    both_null = (df[column_name].isnull() & df[col].isnull()).sum()
                    both_not_null = (df[column_name].notnull() & df[col].notnull()).sum()
                    
                    print(f"   ğŸ“Š {col}:")
                    print(f"      ğŸ”— Both null: {both_null:,}")
                    print(f"      âœ… Both populated: {both_not_null:,}")
                    
                    # Data type and basic stats
                    print(f"      ğŸ“ˆ Other column stats: {len(other_data):,} non-null, {df[col].dtype}")
            except:
                pass
    
    # Company namespace correlation
    company_columns = [col for col in df.columns if col.startswith('company/') and col != column_name]
    high_correlation_cols = []
    
    print(f"\nğŸ¢ Company namespace analysis:")
    for col in company_columns[:5]:  # Check first 5 company columns
        try:
            # Check null pattern correlation
            null_correlation = (df[column_name].isnull() == df[col].isnull()).mean()
            if null_correlation > 0.8:
                high_correlation_cols.append((col, null_correlation))
        except:
            pass
    
    if high_correlation_cols:
        print("   ğŸ”— High null pattern correlation:")
        for col, corr in high_correlation_cols:
            print(f"      ğŸ“Š {col}: {corr:.1%}")
    else:
        print("   âœ… No significant null pattern correlations")
    print()
    
    # 7. NULL DOLDURMA STRATEJÄ°LERÄ°
    print("ğŸ’¡ 7. NULL DOLDURMA STRATEJÄ°LERÄ°")
    print("-" * 30)
    
    if null_percentage > 0:
        print(f"ğŸ“Š Null oranÄ±: %{null_percentage:.1f}")
        
        # Strategy recommendations based on null percentage
        if null_percentage > 80:
            strategy = "ğŸš¨ YÃœKSEK NULL - SÃ¼tun silinmeyi dÃ¼ÅŸÃ¼nÃ¼n"
        elif null_percentage > 50:
            strategy = "âš ï¸ ORTA-YÃœKSEK NULL - Ã–zel strateji gerekli"
        elif null_percentage > 20:
            strategy = "ğŸ”§ ORTA NULL - Doldurma stratejisi uygulanabilir"
        else:
            strategy = "âœ… DÃœÅÃœK NULL - Minimal intervention"
        
        print(f"ğŸ“‹ Strateji Ã¶nerisi: {strategy}")
        
        # Specific recommendations for URN data
        print("\nğŸ¯ URN-specific doldurma Ã¶nerileri:")
        print("   1. ğŸ”— 'UNKNOWN_URN' placeholder value")
        print("   2. ğŸ“Š Company/following relationship based imputation")
        print("   3. ğŸ—‘ï¸ Row deletion if URN critical for analysis")
        print("   4. ğŸ”„ API re-fetch if possible")
        
        # Business logic recommendations
        print("\nğŸ’¼ Business logic recommendations:")
        print("   â€¢ URN null = User not following company")
        print("   â€¢ Could create 'NOT_FOLLOWING' category")
        print("   â€¢ Pattern analysis with other following columns")
    else:
        print("âœ… No null values detected!")
    print()
    
    # 8. OPTIMIZATION Ã–NERÄ°LERÄ°
    print("ğŸš€ 8. OPTIMIZATION Ã–NERÄ°LERÄ° VE EYLEM PLANI")
    print("-" * 45)
    
    recommendations = []
    
    # Data type optimization
    if col_data.dtype == 'object' and len(non_null_data) > 0:
        memory_current = col_data.memory_usage(deep=True) / 1024**2
        recommendations.append(f"ğŸ’¾ Current memory usage: {memory_current:.2f} MB")
        
        # Category conversion possibility
        if unique_count < 1000:
            potential_savings = memory_current * 0.3  # Estimate 30% savings
            recommendations.append(f"ğŸ”§ Category conversion potential: -{potential_savings:.2f} MB")
    
    # Format standardization
    if len(non_null_data) > 0:
        if valid_urn_pct < 95:
            recommendations.append("ğŸ”§ URN format standardization needed")
        
        if case_variations:
            recommendations.append("ğŸ”¤ Case normalization recommended")
    
    # Business value assessment
    if unique_count == 1:
        recommendations.append("ğŸš¨ CRITICAL: Zero variance - DELETE column")
    elif unique_count < 10:
        recommendations.append("âš ï¸ Low variance - Question business value")
    elif null_percentage > 80:
        recommendations.append("ğŸš¨ High null ratio - Consider deletion")
    else:
        recommendations.append("âœ… Good analytical potential")
    
    # Final recommendations
    print("ğŸ“‹ Final Recommendations:")
    if recommendations:
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
    
    # Action priority
    print(f"\nğŸ¯ ACTION PRIORITY:")
    if unique_count == 1:
        priority = "ğŸ”´ HIGH - Delete immediately (zero variance)"
    elif null_percentage > 80:
        priority = "ğŸŸ  MEDIUM-HIGH - Evaluate deletion vs imputation"
    elif null_percentage > 50:
        priority = "ğŸŸ¡ MEDIUM - Implement imputation strategy"
    else:
        priority = "ğŸŸ¢ LOW - Monitor and optimize format"
    
    print(f"   {priority}")
    
    print("\n" + "=" * 80)
    
    return {
        'column_name': column_name,
        'total_rows': total_rows,
        'null_count': null_count,
        'null_percentage': null_percentage,
        'unique_count': unique_count if len(non_null_data) > 0 else 0,
        'data_type': str(col_data.dtype),
        'urn_format_valid': valid_urn_pct if len(non_null_data) > 0 else 0,
        'recommendations': recommendations
    }

def main():
    """Ana analiz fonksiyonu"""
    
    print("ğŸ” LinkedIn Jobs Dataset - preDashFollowingInfoUrn Analysis")
    print("=" * 65)
    
    # Dataset'i yÃ¼kle
    try:
        df = pd.read_csv('linkedin_jobs_dataset_cleaned_columns.csv')
        print(f"âœ… Cleaned dataset yÃ¼klendi: {len(df):,} satÄ±r, {len(df.columns)} sÃ¼tun")
        print()
    except Exception as e:
        print(f"âŒ HATA: Dataset yÃ¼klenemedi - {e}")
        return
    
    # Analiz gerÃ§ekleÅŸtir
    result = analyze_predash_following_info_urn(df)
    
    if result:
        print("ğŸ¯ ANALÄ°Z TAMAMLANDI!")
        print(f"ğŸ“Š SonuÃ§: {result['unique_count']:,} benzersiz deÄŸer, %{result['null_percentage']:.1f} null")
        
        # Critical findings
        if result['unique_count'] <= 1:
            print("ğŸš¨ KRÄ°TÄ°K: Zero/minimal variance detected!")
        elif result['null_percentage'] > 80:
            print("âš ï¸ UYARI: Very high null percentage!")
        elif result['urn_format_valid'] < 50:
            print("âš ï¸ UYARI: Poor URN format consistency!")
        else:
            print("âœ… SÃ¼tun genel olarak analiz iÃ§in uygun gÃ¶rÃ¼nÃ¼yor")

if __name__ == "__main__":
    main() 