#!/usr/bin/env python3
"""
LinkedIn Jobs Dataset - Comprehensive Link Column Analysis

Bu script link sÃ¼tununu kapsamlÄ± olarak analiz eder:
1. SÃ¼tun karakteristikleri ve URL pattern analizi
2. Veri tutarlÄ±lÄ±ÄŸÄ± ve format standardizasyonu
3. Veri tipi uyumluluÄŸu kontrolÃ¼
4. BoÅŸ alan yoÄŸunluÄŸu ve doldurma stratejileri
5. Benzer sÃ¼tunlarla redundancy kontrolÃ¼
6. Ä°ÅŸ deÄŸeri ve insight potansiyeli
"""

import pandas as pd
import numpy as np
import re
from urllib.parse import urlparse, parse_qs
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def load_dataset():
    """Dataset'i yÃ¼kle"""
    try:
        df = pd.read_csv('linkedin_jobs_cleaned_no_redundant_workplace.csv')
        print(f"âœ… Dataset baÅŸarÄ±yla yÃ¼klendi: {len(df):,} kayÄ±t, {len(df.columns)} sÃ¼tun")
        return df
    except Exception as e:
        print(f"âŒ Dataset yÃ¼kleme hatasÄ±: {e}")
        return None

def analyze_basic_characteristics(df, target_col):
    """Link sÃ¼tununun temel karakteristikleri"""
    print("=" * 80)
    print(f"ğŸ”— LINK SÃœTUN ANALÄ°ZÄ°: {target_col}")
    print("=" * 80)
    
    print(f"ğŸ“Š TEMEL KARAKTERÄ°STÄ°KLER:")
    print(f"   â€¢ SÃ¼tun AdÄ±: {target_col}")
    print(f"   â€¢ Veri Tipi: {df[target_col].dtype}")
    print(f"   â€¢ Toplam KayÄ±t: {len(df):,}")
    print(f"   â€¢ Null Olmayan: {df[target_col].count():,}")
    print(f"   â€¢ Null DeÄŸer: {df[target_col].isnull().sum():,}")
    print(f"   â€¢ Veri TamlÄ±ÄŸÄ±: {(df[target_col].count()/len(df)*100):.2f}%")
    print(f"   â€¢ Benzersiz URL: {df[target_col].nunique():,}")
    print(f"   â€¢ Benzersizlik OranÄ±: {(df[target_col].nunique()/df[target_col].count()*100):.2f}%")
    
    print(f"\nğŸŒ SÃœTUN TEMSIL ETTÄ°ÄÄ° KAVRAM:")
    print(f"   â€¢ LinkedIn iÅŸ ilanlarÄ±nÄ±n direkt URL'leri")
    print(f"   â€¢ Her URL benzersiz bir job posting'e iÅŸaret eder")
    print(f"   â€¢ Ä°ÅŸ arayanlarÄ±n ilanlara eriÅŸim noktasÄ±")
    print(f"   â€¢ LinkedIn platformunda canonical link yapÄ±sÄ±")
    
    return {
        'total_records': len(df),
        'non_null_count': df[target_col].count(),
        'null_count': df[target_col].isnull().sum(),
        'unique_count': df[target_col].nunique(),
        'data_type': str(df[target_col].dtype)
    }

def analyze_url_patterns(df, target_col):
    """URL pattern ve format analizi"""
    print(f"\nğŸ” URL PATTERN ANALÄ°ZÄ°:")
    print("-" * 50)
    
    # Sample URLs
    sample_urls = df[target_col].dropna().head(10).tolist()
    print(f"ğŸ“‹ Ã–RNEK URL'LER:")
    for i, url in enumerate(sample_urls, 1):
        print(f"   {i:2d}. {url}")
    
    # URL components analysis
    print(f"\nğŸ—ï¸  URL YAPISI ANALÄ°ZÄ°:")
    non_null_urls = df[target_col].dropna()
    
    if len(non_null_urls) > 0:
        # Protocol analysis
        protocols = []
        domains = []
        paths = []
        
        for url in non_null_urls.head(1000):  # Sample ilk 1000 URL
            try:
                parsed = urlparse(str(url))
                protocols.append(parsed.scheme)
                domains.append(parsed.netloc)
                paths.append(parsed.path)
            except:
                continue
        
        print(f"   â€¢ Protocol DaÄŸÄ±lÄ±mÄ±:")
        protocol_counts = Counter(protocols)
        for protocol, count in protocol_counts.most_common():
            print(f"     - {protocol}: {count:,} adet")
        
        print(f"   â€¢ Domain DaÄŸÄ±lÄ±mÄ±:")
        domain_counts = Counter(domains)
        for domain, count in domain_counts.most_common(5):
            print(f"     - {domain}: {count:,} adet")
        
        # Path pattern analysis
        print(f"   â€¢ Path Pattern Ã–rnekleri:")
        unique_paths = list(set(paths))[:5]
        for path in unique_paths:
            print(f"     - {path}")
    
    return protocols, domains, paths

def check_data_consistency(df, target_col):
    """Veri tutarlÄ±lÄ±ÄŸÄ± kontrolÃ¼"""
    print(f"\nâš ï¸  VERÄ° TUTARLILIK KONTROLÃœ:")
    print("-" * 50)
    
    non_null_urls = df[target_col].dropna()
    issues = []
    
    # URL format validation
    invalid_urls = []
    valid_url_pattern = re.compile(
        r'^https?://'  # http:// or https://
        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)$', re.IGNORECASE)
    
    for idx, url in non_null_urls.items():
        url_str = str(url).strip()
        if not valid_url_pattern.match(url_str):
            invalid_urls.append((idx, url_str))
            if len(invalid_urls) <= 5:  # Ä°lk 5 Ã¶rnek
                issues.append(f"GeÃ§ersiz URL format: {url_str}")
    
    print(f"ğŸ“Š TUTARLILIK SONUÃ‡LARI:")
    print(f"   â€¢ Toplam incelenen URL: {len(non_null_urls):,}")
    print(f"   â€¢ GeÃ§ersiz URL format: {len(invalid_urls):,}")
    print(f"   â€¢ Format tutarlÄ±lÄ±ÄŸÄ±: {((len(non_null_urls)-len(invalid_urls))/len(non_null_urls)*100):.2f}%")
    
    if issues:
        print(f"\nğŸš¨ TESPÄ°T EDÄ°LEN SORUNLAR:")
        for issue in issues[:10]:  # Ä°lk 10 sorun
            print(f"   â€¢ {issue}")
    
    # LinkedIn domain consistency
    linkedin_domains = ['linkedin.com', 'www.linkedin.com', 'tr.linkedin.com']
    linkedin_count = 0
    other_domains = []
    
    for url in non_null_urls.head(1000):
        try:
            domain = urlparse(str(url)).netloc.lower()
            if any(ld in domain for ld in linkedin_domains):
                linkedin_count += 1
            else:
                other_domains.append(domain)
        except:
            continue
    
    print(f"\nğŸ¢ DOMAIN TUTARLILIK:")
    print(f"   â€¢ LinkedIn domain'leri: {linkedin_count:,}")
    print(f"   â€¢ DiÄŸer domain'ler: {len(other_domains):,}")
    
    if other_domains:
        print(f"   â€¢ Beklenmeyen domain Ã¶rnekleri:")
        for domain in list(set(other_domains))[:5]:
            print(f"     - {domain}")
    
    return len(invalid_urls), linkedin_count, other_domains

def analyze_null_density(df, target_col):
    """BoÅŸ alan yoÄŸunluÄŸu analizi"""
    print(f"\nâ“ BOÅ ALAN YOÄUNLUÄU ANALÄ°ZÄ°:")
    print("-" * 50)
    
    null_count = df[target_col].isnull().sum()
    null_percentage = (null_count / len(df)) * 100
    
    print(f"ğŸ“Š BOÅ VERÄ° Ä°STATÄ°STÄ°KLERÄ°:")
    print(f"   â€¢ Null deÄŸer sayÄ±sÄ±: {null_count:,}")
    print(f"   â€¢ Null oranÄ±: %{null_percentage:.2f}")
    print(f"   â€¢ Dolu veri oranÄ±: %{100-null_percentage:.2f}")
    
    # Kritik seviye deÄŸerlendirmesi
    if null_percentage == 0:
        print(f"   âœ… MÃ¼kemmel: HiÃ§ boÅŸ veri yok")
    elif null_percentage < 5:
        print(f"   âœ… Ä°yi: DÃ¼ÅŸÃ¼k null oranÄ±")
    elif null_percentage < 15:
        print(f"   âš ï¸  Orta: Kabul edilebilir null oranÄ±")
    else:
        print(f"   ğŸš¨ YÃ¼ksek: Kritik null oranÄ±")
    
    print(f"\nğŸ’¡ BOÅ VERÄ° DOLDURMA STRATEJÄ°LERÄ°:")
    if null_count > 0:
        print(f"   ğŸ¯ Ã–NERÄ°LEN STRATEJÄ°LER:")
        print(f"      1. Ä°ÅŸ ID'sinden URL tÃ¼retme:")
        print(f"         - Pattern: https://linkedin.com/jobs/view/{{job_id}}")
        print(f"         - 'id' sÃ¼tununu kullanarak otomatik URL oluÅŸturma")
        print(f"      2. Company URL'lerinden tÃ¼retme:")
        print(f"         - Company bilgisi + job title kombinasyonu")
        print(f"      3. Default LinkedIn job search URL:")
        print(f"         - Genel LinkedIn jobs sayfasÄ±na yÃ¶nlendirme")
        print(f"      4. Null bÄ±rakma:")
        print(f"         - EÄŸer URL kritik deÄŸilse null kabul edilebilir")
    else:
        print(f"   âœ… Doldurma gerekmez: Tam veri mevcut")
    
    return null_count, null_percentage

def check_data_type_compatibility(df, target_col):
    """Veri tipi uyumluluÄŸu kontrolÃ¼"""
    print(f"\nğŸ”§ VERÄ° TÄ°PÄ° UYUMLULUK KONTROLÃœ:")
    print("-" * 50)
    
    current_dtype = df[target_col].dtype
    print(f"ğŸ“Š MEVCUT VERÄ° TÄ°PÄ°: {current_dtype}")
    
    # URL'ler iÃ§in ideal veri tipi string/object
    print(f"\nâœ… VERÄ° TÄ°PÄ° UYGUNLUK DEÄERLENDÄ°RMESÄ°:")
    if current_dtype == 'object':
        print(f"   âœ… UYGUN: 'object' tipi URL'ler iÃ§in ideal")
        print(f"   ğŸ“‹ GerekÃ§eler:")
        print(f"      â€¢ String veri saklama uygun")
        print(f"      â€¢ Variable length URL'ler desteklenir")
        print(f"      â€¢ Null value'lar uygun ÅŸekilde handle edilir")
        print(f"      â€¢ Memory efficiency makul seviyede")
        
        recommendation = "KORUNMALI"
    else:
        print(f"   âŒ UYGUNSUZ: '{current_dtype}' tipi URL'ler iÃ§in uygun deÄŸil")
        print(f"   ğŸ”„ Ã–NERÄ°LEN VERÄ° TÄ°PÄ°: 'object' (string)")
        print(f"   ğŸ“‹ DÃ¶nÃ¼ÅŸÃ¼m gerekÃ§eleri:")
        print(f"      â€¢ URL'ler text data olarak saklanmalÄ±")
        print(f"      â€¢ String operations iÃ§in optimize edilmeli")
        
        recommendation = "OBJECT'E Ã‡EVRÄ°LMELÄ°"
    
    print(f"\nğŸ¯ Ã–NERÄ°: Veri tipi {recommendation}")
    
    return current_dtype, recommendation

def analyze_format_standardization(df, target_col):
    """Format standardizasyonu analizi"""
    print(f"\nğŸ“ FORMAT STANDARDÄ°ZASYONU ANALÄ°ZÄ°:")
    print("-" * 50)
    
    non_null_urls = df[target_col].dropna()
    standardization_issues = []
    
    # Case sensitivity analizi
    case_issues = []
    protocol_variations = []
    domain_variations = []
    
    for url in non_null_urls.head(1000):
        url_str = str(url).strip()
        
        # Protocol case variations
        if url_str.startswith('HTTP://'):
            protocol_variations.append('HTTP://')
        elif url_str.startswith('HTTPS://'):
            protocol_variations.append('HTTPS://')
        elif url_str.startswith('http://'):
            protocol_variations.append('http://')
        elif url_str.startswith('https://'):
            protocol_variations.append('https://')
        
        # Domain case variations
        try:
            domain = urlparse(url_str).netloc
            if domain != domain.lower():
                domain_variations.append((domain, domain.lower()))
        except:
            continue
    
    print(f"ğŸ“Š STANDARDIZASYON DURUMU:")
    
    # Protocol standardization
    protocol_counts = Counter(protocol_variations)
    print(f"   â€¢ Protocol VaryasyonlarÄ±:")
    for protocol, count in protocol_counts.items():
        print(f"     - {protocol}: {count:,} adet")
    
    # Domain case issues
    if domain_variations:
        print(f"   â€¢ Domain case issues: {len(domain_variations):,} adet")
        for original, corrected in domain_variations[:3]:
            print(f"     - '{original}' â†’ '{corrected}'")
    
    # Standardization recommendations
    print(f"\nğŸ’¡ STANDARDÄ°ZASYON Ã–NERÄ°LERÄ°:")
    
    standardization_needed = False
    
    if len(protocol_counts) > 1:
        standardization_needed = True
        print(f"   ğŸ”„ Protocol Standardization:")
        print(f"      â€¢ TÃ¼m URL'leri 'https://' ile baÅŸlat")
        print(f"      â€¢ HTTP protokollerini HTTPS'e dÃ¶nÃ¼ÅŸtÃ¼r")
        print(f"      â€¢ BÃ¼yÃ¼k harf protocol'leri kÃ¼Ã§Ã¼k harfe Ã§evir")
    
    if domain_variations:
        standardization_needed = True
        print(f"   ğŸ”„ Domain Standardization:")
        print(f"      â€¢ TÃ¼m domain'leri lowercase'e Ã§evir")
        print(f"      â€¢ URL parsing tutarlÄ±lÄ±ÄŸÄ±nÄ± artÄ±r")
    
    if not standardization_needed:
        print(f"   âœ… Format zaten standardize")
    
    return standardization_needed, protocol_counts, domain_variations

def check_column_multiplication(df, target_col):
    """SÃ¼tun Ã§oÄŸullaÅŸmasÄ± kontrolÃ¼"""
    print(f"\nğŸ”¢ SÃœTUN Ã‡OÄULLAÅMASI KONTROLÃœ:")
    print("-" * 50)
    
    # Link ile ilgili sÃ¼tunlarÄ± ara
    link_related_columns = []
    url_related_columns = []
    
    for col in df.columns:
        col_lower = col.lower()
        if 'link' in col_lower or 'url' in col_lower:
            link_related_columns.append(col)
        # Company URL'leri de kontrol et
        if 'company' in col_lower and any(keyword in col_lower for keyword in ['url', 'link', 'website']):
            url_related_columns.append(col)
    
    print(f"ğŸ“‹ Ä°LGÄ°LÄ° SÃœTUNLAR:")
    print(f"   â€¢ Link/URL sÃ¼tunlarÄ±: {len(link_related_columns)} adet")
    for col in link_related_columns:
        unique_count = df[col].nunique()
        null_pct = (df[col].isnull().sum() / len(df)) * 100
        print(f"     - {col}: {unique_count:,} benzersiz, %{null_pct:.1f} null")
    
    if url_related_columns:
        print(f"   â€¢ Company URL sÃ¼tunlarÄ±: {len(url_related_columns)} adet")
        for col in url_related_columns:
            unique_count = df[col].nunique()
            null_pct = (df[col].isnull().sum() / len(df)) * 100
            print(f"     - {col}: {unique_count:,} benzersiz, %{null_pct:.1f} null")
    
    # Ã‡oÄŸullaÅŸma analizi
    total_url_columns = len(link_related_columns) + len(url_related_columns)
    
    print(f"\nğŸ¯ Ã‡OÄULLAÅMA DEÄERLENDÄ°RMESÄ°:")
    if total_url_columns == 1:
        print(f"   âœ… Tek sÃ¼tun: Ã‡oÄŸullaÅŸma yok")
        multiplication_analysis = "NO_MULTIPLICATION"
    elif total_url_columns <= 3:
        print(f"   âš ï¸  Az sayÄ±da URL sÃ¼tunu: Kontrol edilmeli")
        multiplication_analysis = "MINIMAL_MULTIPLICATION"
    else:
        print(f"   ğŸš¨ Ã‡ok sayÄ±da URL sÃ¼tunu: Consolidation gerekebilir")
        multiplication_analysis = "SIGNIFICANT_MULTIPLICATION"
    
    return link_related_columns, url_related_columns, multiplication_analysis

def find_similar_columns(df, target_col):
    """Benzer sÃ¼tunlarla redundancy kontrolÃ¼"""
    print(f"\nğŸ”— BENZER SÃœTUNLAR Ä°LE REDUNDANCY KONTROLÃœ:")
    print("-" * 50)
    
    target_unique_count = df[target_col].nunique()
    similar_columns = []
    
    # ID sÃ¼tunlarÄ± ile karÅŸÄ±laÅŸtÄ±rma
    id_columns = [col for col in df.columns if 'id' in col.lower()]
    
    print(f"ğŸ“Š REDUNDANCY ANALÄ°ZÄ°:")
    print(f"   â€¢ {target_col} benzersiz deÄŸer: {target_unique_count:,}")
    
    # ID sÃ¼tunlarÄ± ile correlation
    print(f"\nğŸ†” ID SÃœTUNLARI Ä°LE KORELASYON:")
    for col in id_columns:
        col_unique = df[col].nunique()
        print(f"   â€¢ {col}: {col_unique:,} benzersiz deÄŸer")
        
        # AynÄ± benzersizlik seviyesi kontrolÃ¼
        if col_unique == target_unique_count:
            similar_columns.append({
                'column': col,
                'unique_count': col_unique,
                'potential_redundancy': 'HIGH'
            })
            print(f"     ğŸš¨ AYNI BENZERSÄ°ZLÄ°K SAYISI - Potansiyel redundancy!")
    
    # URL iÃ§eriÄŸinden ID Ã§Ä±karma analizi
    print(f"\nğŸ” URL'LERDEN ID Ã‡IKARMA ANALÄ°ZÄ°:")
    sample_urls = df[target_col].dropna().head(10)
    
    extracted_ids = []
    for url in sample_urls:
        # LinkedIn job URL pattern: /jobs/view/JOBID
        match = re.search(r'/jobs/view/(\d+)', str(url))
        if match:
            job_id = match.group(1)
            extracted_ids.append(job_id)
    
    if extracted_ids:
        print(f"   â€¢ URL'lerden Ã§Ä±karÄ±lan ID Ã¶rnekleri:")
        for i, job_id in enumerate(extracted_ids[:5], 1):
            print(f"     {i}. {job_id}")
        
        # ID sÃ¼tunu ile karÅŸÄ±laÅŸtÄ±rma
        if 'id' in df.columns:
            sample_ids = df['id'].head(10).astype(str).tolist()
            matches = len(set(extracted_ids) & set(sample_ids))
            print(f"   â€¢ 'id' sÃ¼tunu ile eÅŸleÅŸme: {matches}/{len(extracted_ids)} Ã¶rnekte")
            
            if matches > len(extracted_ids) * 0.8:  # %80 Ã¼zeri eÅŸleÅŸme
                print(f"     ğŸš¨ YÃœKSEK REDUNDANCY: URL'ler 'id' sÃ¼tunundan tÃ¼retilebilir!")
                similar_columns.append({
                    'column': 'id',
                    'unique_count': df['id'].nunique(),
                    'potential_redundancy': 'DERIVABLE',
                    'derivation_rule': 'https://linkedin.com/jobs/view/{id}'
                })
    
    # SonuÃ§ Ã¶nerileri
    print(f"\nğŸ’¡ REDUNDANCY Ã–NERÄ°LERÄ°:")
    if similar_columns:
        for sim in similar_columns:
            print(f"   ğŸ”„ {sim['column']} ile redundancy:")
            if sim.get('derivation_rule'):
                print(f"      â€¢ TÃ¼retme kuralÄ±: {sim['derivation_rule']}")
                print(f"      â€¢ Ã–NERÄ°: Link sÃ¼tununu sil, ihtiyaÃ§ta ID'den tÃ¼ret")
            else:
                print(f"      â€¢ DetaylÄ± analiz gerekli")
    else:
        print(f"   âœ… Belirgin redundancy tespit edilmedi")
    
    return similar_columns

def generate_insights_and_recommendations(df, target_col, analysis_results):
    """Insights ve Ã¶neriler oluÅŸtur"""
    print(f"\nğŸ’¡ INSIGHTS VE STRATEJÄ°K Ã–NERÄ°LER:")
    print("=" * 60)
    
    basic_stats = analysis_results['basic_stats']
    null_info = analysis_results['null_analysis']
    similar_cols = analysis_results['similar_columns']
    
    print(f"ğŸ¯ {target_col} SÃœTUNU HAKKINDA:")
    print(f"   â€¢ Temsil ettiÄŸi: LinkedIn job posting URL'leri")
    print(f"   â€¢ Veri kalitesi: %{(basic_stats['non_null_count']/basic_stats['total_records']*100):.1f} tamlÄ±k")
    print(f"   â€¢ Benzersizlik: %{(basic_stats['unique_count']/basic_stats['non_null_count']*100):.1f}")
    
    # Business value assessment
    print(f"\nğŸ“Š Ä°Å DEÄERÄ° DEÄERLENDÄ°RMESÄ°:")
    
    # URL'lerin business value'su
    if basic_stats['unique_count'] / basic_stats['non_null_count'] > 0.95:
        print(f"   âœ… YÃ¼ksek deÄŸer: Her URL benzersiz job posting")
        print(f"   ğŸ”— KullanÄ±m alanlarÄ±:")
        print(f"      â€¢ Direkt iÅŸ ilanÄ±na eriÅŸim")
        print(f"      â€¢ External referencing")
        print(f"      â€¢ Web scraping iÃ§in endpoint")
        print(f"      â€¢ User experience enhancement")
    else:
        print(f"   âš ï¸  Orta deÄŸer: BazÄ± URL'ler tekrarlÄ±yor")
    
    # Redundancy deÄŸerlendirmesi
    if similar_cols:
        print(f"\nğŸš¨ REDUNDANCY BULGULARI:")
        for sim in similar_cols:
            if sim.get('derivation_rule'):
                print(f"   â€¢ {sim['column']} sÃ¼tunÄ±ndan tÃ¼retilebilir")
                print(f"   â€¢ Potansiyel bellek tasarrufu var")
                print(f"   â€¢ Ã–NERI: TÃ¼retme metodunu deÄŸerlendir")
    
    # Optimizasyon Ã¶nerileri
    print(f"\nğŸ”§ OPTÄ°MÄ°ZASYON Ã–NERÄ°LERÄ°:")
    
    # 1. Null handling
    if null_info['null_count'] > 0:
        print(f"   1ï¸âƒ£ Null DeÄŸer Optimizasyonu:")
        print(f"      â€¢ {null_info['null_count']:,} adet null URL var")
        print(f"      â€¢ 'id' sÃ¼tununu kullanarak tÃ¼retme mÃ¼mkÃ¼n")
        print(f"      â€¢ URL pattern: https://linkedin.com/jobs/view/{{id}}")
    
    # 2. Storage optimization
    print(f"   2ï¸âƒ£ Storage Optimizasyonu:")
    if similar_cols and any(sim.get('derivation_rule') for sim in similar_cols):
        print(f"      â€¢ URL'leri runtime'da tÃ¼retme")
        print(f"      â€¢ Base URL + ID kombinasyonu")
        print(f"      â€¢ Memory footprint azaltma")
    else:
        print(f"      â€¢ Mevcut format optimize edilmiÅŸ gÃ¶rÃ¼nÃ¼yor")
    
    # 3. Data quality improvements
    print(f"   3ï¸âƒ£ Veri Kalitesi Ä°yileÅŸtirmeleri:")
    print(f"      â€¢ URL format validation")
    print(f"      â€¢ Protocol standardization (https://)")
    print(f"      â€¢ Domain consistency kontrolÃ¼")
    
    # 4. Business intelligence enhancements
    print(f"   4ï¸âƒ£ Business Intelligence GeliÅŸtirmeleri:")
    print(f"      â€¢ URL analytics (click tracking)")
    print(f"      â€¢ Geographic link analysis")
    print(f"      â€¢ Referral source tracking")
    
    # Kritik kararlar
    print(f"\nğŸ¯ KRÄ°TÄ°K KARARLAR:")
    
    if similar_cols and any(sim.get('derivation_rule') for sim in similar_cols):
        print(f"   âš ï¸  URL SÃœTUNU ELÄ°MÄ°NASYONU DEÄERLENDÄ°RÄ°LEBÄ°LÄ°R:")
        print(f"      â€¢ Avantajlar: Memory tasarrufu, simple structure")
        print(f"      â€¢ Dezavantajlar: Runtime computation, dependency on ID")
        print(f"      â€¢ Ã–neri: Business requirement'lara gÃ¶re karar ver")
    else:
        print(f"   âœ… URL SÃœTUNU KORUNMALI:")
        print(f"      â€¢ Benzersiz business value saÄŸlÄ±yor")
        print(f"      â€¢ Direct access iÃ§in kritik")
        print(f"      â€¢ Alternative yok")

def main():
    """Ana analiz fonksiyonu"""
    print("ğŸš€ LinkedIn Jobs Dataset - Link SÃ¼tunu KapsamlÄ± Analizi")
    print("=" * 80)
    
    # Dataset yÃ¼kle
    df = load_dataset()
    if df is None:
        return
    
    target_col = 'link'
    
    # SÃ¼tun varlÄ±ÄŸÄ±nÄ± kontrol et
    if target_col not in df.columns:
        print(f"âŒ Hata: '{target_col}' sÃ¼tunu bulunamadÄ±!")
        available_link_cols = [col for col in df.columns if 'link' in col.lower() or 'url' in col.lower()]
        if available_link_cols:
            print(f"Mevcut benzer sÃ¼tunlar: {available_link_cols}")
        return
    
    # KapsamlÄ± analiz
    try:
        # 1. Temel karakteristikler
        basic_stats = analyze_basic_characteristics(df, target_col)
        
        # 2. URL pattern analizi
        protocols, domains, paths = analyze_url_patterns(df, target_col)
        
        # 3. Veri tutarlÄ±lÄ±ÄŸÄ±
        invalid_count, linkedin_count, other_domains = check_data_consistency(df, target_col)
        
        # 4. Null density
        null_count, null_percentage = analyze_null_density(df, target_col)
        
        # 5. Data type compatibility
        current_dtype, type_recommendation = check_data_type_compatibility(df, target_col)
        
        # 6. Format standardization
        standardization_needed, protocol_counts, domain_vars = analyze_format_standardization(df, target_col)
        
        # 7. Column multiplication
        link_cols, url_cols, multiplication = check_column_multiplication(df, target_col)
        
        # 8. Similar columns redundancy
        similar_columns = find_similar_columns(df, target_col)
        
        # 9. Comprehensive insights
        analysis_results = {
            'basic_stats': basic_stats,
            'null_analysis': {'null_count': null_count, 'null_percentage': null_percentage},
            'similar_columns': similar_columns,
            'standardization': standardization_needed,
            'multiplication': multiplication
        }
        
        generate_insights_and_recommendations(df, target_col, analysis_results)
        
        print(f"\nâœ… Link sÃ¼tunu kapsamlÄ± analizi tamamlandÄ±!")
        
    except Exception as e:
        print(f"âŒ Analiz hatasÄ±: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 