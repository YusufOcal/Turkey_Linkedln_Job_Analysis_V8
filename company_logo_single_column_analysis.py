#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LinkedIn Jobs Dataset - company/logo Single Column Deep Analysis
Kapsamlƒ± tek s√ºtun analizi: format, i√ßerik, optimize edilebilirlik
"""

import pandas as pd
import numpy as np
from collections import Counter
import re
import urllib.parse
from urllib.parse import urlparse
import warnings
warnings.filterwarnings('ignore')

def analyze_company_logo_column(df):
    """company/logo s√ºtunu i√ßin kapsamlƒ± analiz"""
    
    print("üè¢ LINKEDIN JOBS DATASET - COMPANY/LOGO COLUMN ANALYSIS")
    print("=" * 65)
    
    column_name = 'company/logo'
    
    # Check if column exists
    if column_name not in df.columns:
        print(f"‚ùå HATA: {column_name} s√ºtunu bulunamadƒ±!")
        return None
    
    col_data = df[column_name]
    
    # 1. S√úTUN TEMSƒ∞Lƒ∞ VE ANLAMI
    print("üìã 1. S√úTUN TEMSƒ∞Lƒ∞ VE ANLAMI")
    print("-" * 35)
    
    column_info = {
        'name': column_name,
        'purpose': 'Company Logo URL/Path Storage',
        'content_type': 'URL/URI pointing to company logo image',
        'business_value': 'Visual branding, company identification, UI/UX enhancement',
        'expected_format': 'HTTP/HTTPS URLs or relative paths to image files',
        'source': 'LinkedIn Company API - Logo image reference',
        'data_usage': 'Frontend display, company branding, visual analytics',
        'expected_file_types': 'JPG, PNG, SVG, WebP image formats'
    }
    
    print(f"üè¢ {column_info['name']}:")
    print(f"   üìù Purpose: {column_info['purpose']}")
    print(f"   üíº Content Type: {column_info['content_type']}")
    print(f"   üéØ Business Value: {column_info['business_value']}")
    print(f"   üìä Expected Format: {column_info['expected_format']}")
    print(f"   üîó Source: {column_info['source']}")
    print(f"   üé® Usage: {column_info['data_usage']}")
    print(f"   üì∏ Expected Types: {column_info['expected_file_types']}")
    print()
    
    # 2. TEMEL ƒ∞STATƒ∞STƒ∞KLER
    print("üìä 2. TEMEL ƒ∞STATƒ∞STƒ∞KLER")
    print("-" * 25)
    
    total_rows = len(col_data)
    null_count = col_data.isnull().sum()
    non_null_count = total_rows - null_count
    null_percentage = (null_count / total_rows) * 100
    unique_count = len(col_data.dropna().unique()) if non_null_count > 0 else 0
    memory_usage = col_data.memory_usage(deep=True) / 1024**2
    
    basic_stats = {
        'total_rows': total_rows,
        'null_count': null_count,
        'non_null_count': non_null_count,
        'null_percentage': null_percentage,
        'unique_count': unique_count,
        'memory_mb': memory_usage,
        'data_type': str(col_data.dtype)
    }
    
    print(f"üìä Temel ƒ∞statistikler:")
    print(f"   üìà Toplam kayƒ±t: {total_rows:,}")
    print(f"   ‚ùå Null: {null_count:,} ({null_percentage:.1f}%)")
    print(f"   ‚úÖ Dolu: {non_null_count:,} ({100-null_percentage:.1f}%)")
    print(f"   üéØ Benzersiz: {unique_count:,}")
    print(f"   üíæ Memory: {memory_usage:.2f} MB")
    print(f"   üîß Tip: {col_data.dtype}")
    print()
    
    if non_null_count == 0:
        print("‚ùå HATA: Hi√ß veri yok, analiz durduruluyor!")
        return None
    
    # 3. ƒ∞√áERƒ∞K TUTARLILƒ∞K ANALƒ∞Zƒ∞
    print("üîç 3. ƒ∞√áERƒ∞K TUTARLILƒ∞K ANALƒ∞Zƒ∞")
    print("-" * 35)
    
    non_null_data = col_data.dropna()
    
    # URL format detection
    url_patterns = {
        'http_urls': len(non_null_data[non_null_data.str.startswith('http://', na=False)]),
        'https_urls': len(non_null_data[non_null_data.str.startswith('https://', na=False)]),
        'relative_paths': len(non_null_data[non_null_data.str.startswith('/', na=False)]),
        'data_urls': len(non_null_data[non_null_data.str.startswith('data:', na=False)]),
        'other_protocols': len(non_null_data[~non_null_data.str.startswith(('http://', 'https://', '/', 'data:'), na=False)])
    }
    
    print(f"üîó URL Format Daƒüƒ±lƒ±mƒ±:")
    for format_type, count in url_patterns.items():
        pct = (count / non_null_count) * 100 if non_null_count > 0 else 0
        print(f"   {format_type}: {count:,} ({pct:.1f}%)")
    
    # Domain analysis for URLs
    domains = []
    invalid_urls = 0
    
    for url in non_null_data:
        try:
            if str(url).startswith(('http://', 'https://')):
                parsed = urlparse(str(url))
                if parsed.netloc:
                    domains.append(parsed.netloc.lower())
                else:
                    invalid_urls += 1
            else:
                # Non-URL entries
                pass
        except:
            invalid_urls += 1
    
    if domains:
        domain_counts = Counter(domains)
        print(f"\nüåê Top Domains (URLs only):")
        for domain, count in domain_counts.most_common(5):
            pct = (count / len(domains)) * 100
            print(f"   {domain}: {count:,} ({pct:.1f}%)")
    
    if invalid_urls > 0:
        print(f"\n‚ö†Ô∏è Invalid URL formats: {invalid_urls:,}")
    
    # File extension analysis
    extensions = []
    for url in non_null_data:
        url_str = str(url).lower()
        # Extract extension from URL path
        path = urlparse(url_str).path if url_str.startswith(('http://', 'https://')) else url_str
        if '.' in path:
            ext = path.split('.')[-1].split('?')[0].split('#')[0]  # Remove query params
            if len(ext) <= 5:  # Reasonable extension length
                extensions.append(ext)
    
    if extensions:
        ext_counts = Counter(extensions)
        print(f"\nüì∏ File Extensions:")
        for ext, count in ext_counts.most_common(8):
            pct = (count / len(extensions)) * 100
            print(f"   .{ext}: {count:,} ({pct:.1f}%)")
    
    # 4. VERƒ∞ Tƒ∞Pƒ∞ UYUMLULUƒûU
    print(f"\nüîß 4. VERƒ∞ Tƒ∞Pƒ∞ UYUMLULUƒûU")
    print("-" * 25)
    
    current_type = col_data.dtype
    print(f"üìä Mevcut veri tipi: {current_type}")
    
    # Check if all non-null values are strings
    string_compatible = True
    non_string_count = 0
    
    for val in non_null_data:
        if not isinstance(val, (str, type(np.nan))):
            string_compatible = False
            non_string_count += 1
    
    print(f"üî§ String uyumluluƒüu: {'‚úÖ Uyumlu' if string_compatible else f'‚ùå {non_string_count} uyumsuz'}")
    
    # URL length analysis
    lengths = non_null_data.astype(str).str.len()
    length_stats = {
        'min': lengths.min(),
        'max': lengths.max(),
        'mean': lengths.mean(),
        'median': lengths.median(),
        'std': lengths.std()
    }
    
    print(f"üìè URL Length ƒ∞statistikleri:")
    print(f"   Min: {length_stats['min']} karakter")
    print(f"   Max: {length_stats['max']} karakter") 
    print(f"   Ortalama: {length_stats['mean']:.1f} karakter")
    print(f"   Medyan: {length_stats['median']:.1f} karakter")
    print(f"   Std: {length_stats['std']:.1f}")
    
    # Unusual length detection
    unusual_short = len(lengths[lengths < 10])
    unusual_long = len(lengths[lengths > 200])
    
    if unusual_short > 0:
        print(f"   ‚ö†Ô∏è √áok kƒ±sa URL'ler (<10 char): {unusual_short:,}")
    if unusual_long > 0:
        print(f"   ‚ö†Ô∏è √áok uzun URL'ler (>200 char): {unusual_long:,}")
    
    # 5. BO≈û ALAN YOƒûUNLUKƒ∞ VE DOLDURMA √ñNERƒ∞LERƒ∞
    print(f"\nüìà 5. BO≈û ALAN YOƒûUNLUƒûƒ∞ VE DOLDURMA √ñNERƒ∞LERƒ∞")
    print("-" * 50)
    
    print(f"üìä Null Analizi:")
    print(f"   Null oran: {null_percentage:.1f}%")
    
    if null_percentage > 0:
        # Analyze null pattern by company
        if 'company/name' in df.columns:
            company_logo_stats = df.groupby('company/name').agg({
                column_name: ['count', lambda x: x.isnull().sum()]
            }).round(2)
            
            companies_with_logos = len(df[df['company/name'].notna() & df[column_name].notna()]['company/name'].unique())
            total_companies = len(df[df['company/name'].notna()]['company/name'].unique())
            
            print(f"   üè¢ Logo'lu ≈üirket sayƒ±sƒ±: {companies_with_logos:,}/{total_companies:,}")
            print(f"   üìà ≈ûirket logo coverage: {(companies_with_logos/total_companies)*100:.1f}%")
    
    print(f"\nüí° Doldurma √ñnerileri:")
    if null_percentage > 50:
        print(f"   üö® Y√ºksek null oran ({null_percentage:.1f}%) - DELETE candidate")
    elif null_percentage > 20:
        print(f"   ‚ö†Ô∏è Orta seviye null - Default placeholder √∂nerisi")
        print(f"   üí° Default: 'https://static.licdn.com/company/default-logo.png'")
    else:
        print(f"   ‚úÖ Kabul edilebilir null oran ({null_percentage:.1f}%)")
    
    # 6. FORMAT STANDARDƒ∞ZASYONU
    print(f"\nüîß 6. FORMAT STANDARDƒ∞ZASYONU")
    print("-" * 30)
    
    format_issues = {
        'mixed_protocols': 0,
        'inconsistent_domains': 0,
        'malformed_urls': 0,
        'case_variations': 0
    }
    
    # Protocol analysis
    http_count = url_patterns['http_urls']
    https_count = url_patterns['https_urls']
    
    if http_count > 0 and https_count > 0:
        format_issues['mixed_protocols'] = http_count + https_count
        print(f"‚ö†Ô∏è Mixed protocols detected: {http_count} HTTP, {https_count} HTTPS")
        print(f"   üí° √ñneri: T√ºm HTTP ‚Üí HTTPS conversion")
    
    # Case sensitivity check
    if domains:
        original_domains = set(domains)
        normalized_domains = set(d.lower() for d in domains)
        if len(original_domains) != len(normalized_domains):
            format_issues['case_variations'] = len(original_domains) - len(normalized_domains)
            print(f"‚ö†Ô∏è Case variations detected in domains")
    
    # Malformed URL detection
    malformed_count = 0
    for url in non_null_data.head(100):  # Sample check
        url_str = str(url)
        if url_str.startswith(('http://', 'https://')):
            try:
                parsed = urlparse(url_str)
                if not parsed.netloc or not parsed.scheme:
                    malformed_count += 1
            except:
                malformed_count += 1
    
    if malformed_count > 0:
        estimated_malformed = (malformed_count / 100) * non_null_count
        print(f"‚ö†Ô∏è Estimated malformed URLs: ~{estimated_malformed:.0f}")
        format_issues['malformed_urls'] = estimated_malformed
    
    # Standardization recommendations
    print(f"\nüí° Standardization √ñnerileri:")
    if format_issues['mixed_protocols'] > 0:
        print(f"   1. HTTP ‚Üí HTTPS conversion ({http_count:,} URLs)")
    if format_issues['case_variations'] > 0:
        print(f"   2. Domain case normalization")
    if format_issues['malformed_urls'] > 0:
        print(f"   3. URL validation ve cleanup (~{format_issues['malformed_urls']:.0f} URLs)")
    
    # 7. Dƒ∞ƒûER S√úTUNLARLA BENZERLIK ANALƒ∞Zƒ∞
    print(f"\nüîó 7. Dƒ∞ƒûER S√úTUNLARLA BENZERLIK ANALƒ∞Zƒ∞")
    print("-" * 40)
    
    # Look for similar columns
    potential_similar_columns = []
    for col in df.columns:
        if col != column_name:
            if any(keyword in col.lower() for keyword in ['logo', 'image', 'picture', 'avatar', 'icon', 'media']):
                potential_similar_columns.append(col)
    
    print(f"üîç Potential similar columns:")
    if potential_similar_columns:
        for sim_col in potential_similar_columns:
            sim_null_pct = (df[sim_col].isnull().sum() / len(df)) * 100
            sim_unique = df[sim_col].nunique()
            print(f"   üìã {sim_col}: {sim_unique:,} unique, {sim_null_pct:.1f}% null")
    else:
        print(f"   ‚úÖ No similar columns detected")
    
    # Company name correlation analysis
    if 'company/name' in df.columns:
        # Check if company logos are unique per company
        company_logo_pairs = df[['company/name', column_name]].dropna()
        if len(company_logo_pairs) > 0:
            unique_companies = company_logo_pairs['company/name'].nunique()
            unique_logos = company_logo_pairs[column_name].nunique()
            
            print(f"\nüè¢ Company-Logo Relationship:")
            print(f"   Companies with logos: {unique_companies:,}")
            print(f"   Unique logos: {unique_logos:,}")
            print(f"   Logo/Company ratio: {unique_logos/unique_companies:.2f}")
            
            if unique_logos > unique_companies:
                print(f"   ‚ö†Ô∏è More logos than companies - possible duplicates or variations")
            elif unique_logos < unique_companies * 0.8:
                print(f"   ‚ö†Ô∏è Many companies share logos - possible generic/default logos")
    
    # 8. OPTƒ∞Mƒ∞ZASYON √ñNERƒ∞LERƒ∞
    print(f"\nüí° 8. OPTƒ∞Mƒ∞ZASYON √ñNERƒ∞LERƒ∞")
    print("-" * 30)
    
    optimization_recommendations = []
    
    # Memory optimization
    if unique_count < 1000:
        potential_savings = memory_usage * 0.7
        optimization_recommendations.append(f"üîß Category type conversion (save ~{potential_savings:.2f} MB)")
    
    # URL optimization
    if url_patterns['http_urls'] > 0:
        optimization_recommendations.append(f"üîí HTTP ‚Üí HTTPS migration ({url_patterns['http_urls']:,} URLs)")
    
    # Default value suggestion
    if null_percentage > 10:
        optimization_recommendations.append(f"üé® Default logo implementation for {null_count:,} records")
    
    # Compression possibilities
    if domains and len(set(domains)) < len(domains) * 0.5:
        optimization_recommendations.append(f"üì¶ URL compression (many shared domains)")
    
    print(f"üìã √ñneriler:")
    for i, rec in enumerate(optimization_recommendations, 1):
        print(f"   {i}. {rec}")
    
    # 9. KRƒ∞Tƒ∞K BULGULAR VE KARARLAR
    print(f"\nüéØ 9. KRƒ∞Tƒ∞K BULGULAR VE KARARLAR")
    print("-" * 35)
    
    critical_findings = []
    
    # High null percentage
    if null_percentage > 50:
        critical_findings.append(f"üö® HIGH NULL RATIO ({null_percentage:.1f}%) - DELETE CANDIDATE")
    elif null_percentage > 30:
        critical_findings.append(f"‚ö†Ô∏è MODERATE NULL RATIO ({null_percentage:.1f}%) - NEEDS ATTENTION")
    
    # URL format consistency
    if format_issues['mixed_protocols'] > 100:
        critical_findings.append(f"‚ö†Ô∏è MIXED PROTOCOLS - STANDARDIZATION REQUIRED")
    
    # Business value assessment
    if unique_count < total_rows * 0.1:
        critical_findings.append(f"‚ö†Ô∏è LOW VARIETY - Many duplicate/default logos")
    
    print(f"üîç Critical Findings:")
    if critical_findings:
        for finding in critical_findings:
            print(f"   ‚Ä¢ {finding}")
    else:
        print(f"   ‚úÖ No critical issues detected")
    
    # Final recommendation
    print(f"\nüéØ FINAL RECOMMENDATION:")
    if null_percentage > 60:
        print(f"   üóëÔ∏è RECOMMENDATION: DELETE COLUMN (low business value)")
        action = "DELETE"
    elif null_percentage > 30:
        print(f"   üîß RECOMMENDATION: OPTIMIZE with default values")
        action = "OPTIMIZE"
    else:
        print(f"   ‚úÖ RECOMMENDATION: KEEP and standardize formats")
        action = "KEEP"
    
    print("\n" + "=" * 65)
    
    return {
        'column_name': column_name,
        'basic_stats': basic_stats,
        'url_patterns': url_patterns,
        'format_issues': format_issues,
        'length_stats': length_stats,
        'optimization_recommendations': optimization_recommendations,
        'critical_findings': critical_findings,
        'final_action': action
    }

def main():
    """Ana analiz fonksiyonu"""
    
    print("üîç LinkedIn Jobs Dataset - company/logo Column Analysis")
    print("=" * 60)
    
    # Dataset'i y√ºkle
    try:
        df = pd.read_csv('linkedin_jobs_dataset_optimized_step3.csv')
        print(f"‚úÖ Dataset y√ºklendi: {len(df):,} satƒ±r, {len(df.columns)} s√ºtun")
        print()
    except Exception as e:
        print(f"‚ùå HATA: Dataset y√ºklenemedi - {e}")
        return
    
    # company/logo analizi ger√ßekle≈ütir
    result = analyze_company_logo_column(df)
    
    if result:
        print("üéØ COMPANY/LOGO ANALƒ∞Zƒ∞ TAMAMLANDI!")
        print(f"üìä Final Action: {result['final_action']}")
        
        if result['final_action'] == 'DELETE':
            print("üö® COLUMN DELETION RECOMMENDED!")
        elif result['final_action'] == 'OPTIMIZE':
            print("üîß OPTIMIZATION REQUIRED!")
        else:
            print("‚úÖ COLUMN STRUCTURE ACCEPTABLE!")

if __name__ == "__main__":
    main() 